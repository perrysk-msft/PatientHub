{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "server = 'ace-demo-server.database.windows.net'\n",
    "database = 'PatientHub'\n",
    "username = 'demo'\n",
    "password = 'YourPassword1!'\n",
    "driver= '{ODBC Driver 17 for SQL Server}'\n",
    "connection_string = 'DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password\n",
    "cnxn = pyodbc.connect(connection_string)\n",
    "command = \"SELECT * FROM dbo.Patient\"\n",
    "train = pd.read_sql(command, cnxn, index_col = 'Id')\n",
    "train.drop(['FirstName', 'LastName', 'patient_nbr'], axis = 1, inplace=True)\n",
    "\n",
    "train['discharge_disposition_id'] = train['discharge_disposition_id'].astype('object')\n",
    "train['admission_type_id'] = train['admission_type_id'].astype('object')\n",
    "train['admission_source_id'] = train['admission_source_id'].astype('object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_less_common_levels(train, col_name, in_list):\n",
    "    \n",
    "    \"\"\" Places less common categorical levels into an 'Other' bin.\n",
    "    \n",
    "    :param train: Training set.\n",
    "    :param test: Test set.\n",
    "    :param col_name: Name of column in which to create 'Other' bin.\n",
    "    :param in_list: List of levels NOT to be binned.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # if the level is not in in_list, set it to 'Other'\n",
    "    train.loc[~train[col_name].isin(in_list), col_name] = 'Other'  \n",
    "\n",
    "    # print summary of changes\n",
    "    # print('Train levels after binning:\\n', train[col_name].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_list = list(train['discharge_disposition_id'].value_counts()[:14].index) # first 14 levels contain reasonable amount of info\n",
    "bin_less_common_levels(train, 'discharge_disposition_id', in_list)    # set all other levels to 'Other'\n",
    "\n",
    "in_list = list(train['medical_specialty'].value_counts()[:10].index) # first 10 levels contain reasonable amount of info\n",
    "bin_less_common_levels(train, 'medical_specialty', in_list)    # set all other levels to 'Other'\n",
    "\n",
    "in_list = list(train['diag_1'].value_counts()[:20].index) # first 20 levels contain reasonable amount of info\n",
    "bin_less_common_levels(train, 'diag_1', in_list)    # set all other levels to 'Other'\n",
    "in_list = list(train['diag_2'].value_counts()[:20].index) # first 20 levels contain reasonable amount of info\n",
    "bin_less_common_levels(train, 'diag_2', in_list)    # set all other levels to 'Other'\n",
    "in_list = list(train['diag_3'].value_counts()[:20].index) # first 20 levels contain reasonable amount of info\n",
    "bin_less_common_levels(train, 'diag_3', in_list)    # set all other levels to 'Other'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant column\n",
    "constants = ['acetohexamide', 'examide', 'citoglipton', 'citoglipton', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone']\n",
    "\n",
    "y = 'readmitted' # modeling prediction target\n",
    "\n",
    "# python sets allow for subtraction, lists do not\n",
    "# used here to find the categorical variables that should be dummy-encoded for modeling\n",
    "# convert back to a list for later use \n",
    "encodes = list(set(train.select_dtypes(include=['object']).columns) - set(constants + [y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop the original categorical variables\n",
    "# then join the dummy-encoded versions of the same categorical variables back into the data\n",
    "\n",
    "train = pd.concat([train.drop(encodes, axis=1),\n",
    "                   pd.get_dummies(train[encodes])],\n",
    "                   axis = 1)\n",
    "\n",
    "\n",
    "for name in train.columns: \n",
    "    # use python replace function to replace common '_?' suffix\n",
    "    # use regex to catch everything else\n",
    "    train.rename(columns={name: name.replace('_?', '_q')}, inplace=True)   \n",
    "    train.rename(columns={name: re.sub('[^0-9a-zA-Z]+', '_', name)}, inplace=True)\n",
    "\n",
    "train.drop(['admission_source_id_11', 'admission_source_id_25', 'payer_code_FR', 'admission_source_id_13'], axis = 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train set shape:', train.shape)                                         # test number of columns match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of drops were changed in steps above, must redefine ('-' became '_')\n",
    "constants = ['acetohexamide', 'examide', 'citoglipton', 'citoglipton', 'glimepiride_pioglitazone', 'metformin_rosiglitazone', \n",
    "             'metformin_pioglitazone'] \n",
    "\n",
    "# everything that is not constant, an identifier, or the modeling target will be a modeling input\n",
    "X = [name for name in train.columns if name not in [y] + constants + ['id', 'patient_nbr']]\n",
    "\n",
    "# print summary\n",
    "# print('y =', y)\n",
    "# print('X =', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost treats all columns as numeric - no matter what\n",
    "# any values that can't be converted easily will be NaN - XGBoost does handle NaN elegantly\n",
    "train[X] = train[X].apply(pd.to_numeric, errors='coerce', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert target to numeric value\n",
    "# readmit = NO -> 0\n",
    "# readmit = YES -> 1\n",
    "train.loc[train[y] == 'NO', y] = '0'\n",
    "train.loc[train[y] != '0', y] = '1'\n",
    "train[y] = train[y].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns.values\n",
    "\n",
    "testdf_withID = pd.read_csv(r'F:\\PatientHub\\MLModels\\DiabetesReadmission\\data\\tranformedtestset2.csv', index_col=0)\n",
    "testdf = testdf_withID.drop(columns=['ID'])\n",
    "def diff(first, second):\n",
    "        second = set(second)\n",
    "        return [item for item in first if item not in second]\n",
    "\n",
    "diff(testdf.columns.values, train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(train.columns.values,testdf.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_localexplain(row):\n",
    "    \n",
    "    \"\"\" Summarize local Shapley information. \n",
    "    \n",
    "    :param row: The row to explain from numpy array of Shapley values.\n",
    "    \n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import re\n",
    "    import subprocess\n",
    "    shap_values = np.loadtxt(r'F:\\PatientHub\\MLModels\\DiabetesReadmission\\data\\shap_values.csv', delimiter=',') # load\n",
    "    print('Pre-calculated Shapley values loaded from disk.')        # print confirmation\n",
    "    #this needs to read from AML later\n",
    "    model = pickle.load(open(r'F:\\PatientHub\\MLModels\\DiabetesReadmission\\data\\model.pkl', 'rb'))\n",
    "    \n",
    "    shap_values\n",
    "    # select shapley values for row\n",
    "    # reshape into column vector\n",
    "    # convert to pandas dataframe for easy plotting\n",
    "    \n",
    "    s_df = pd.DataFrame(shap_values[row.index[0], :][:-1].reshape(shap_values.shape[1]-1, 1), \n",
    "                        columns=['Approximate Local Contributions'],\n",
    "                        index=model.feature_names) # must use feature_names for consistent results\n",
    "    print(s_df.shape)\n",
    "    # sort dataframe by shapley values and print values\n",
    "    s_df = s_df.sort_values(by='Approximate Local Contributions')\n",
    "    #print(s_df, '\\n')\n",
    "    return s_df\n",
    "\n",
    "    # plot top positive contributors for this row\n",
    "    #_= s_df.iloc[-5:,:].plot(kind='bar', title='Approximate Local Contributions', legend=False)\n",
    "    \n",
    "    # manually calculate sum of shapley values for row\n",
    "    #print('Shapley sum: ', s_df['Approximate Local Contributions'].sum() + shap_values[row.index[0], -1])\n",
    "    \n",
    "    # manually calculate actual model prediction before application of logit link function\n",
    "    #p = row['predict'].values[0]\n",
    "    #print('Model prediction: ', np.log(p/(1 - p))) # inverse logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_localexplain_df(input_df):\n",
    "    \n",
    "    \"\"\" Summarize local Shapley information. \n",
    "    \n",
    "    :param row: The row to explain from numpy array of Shapley values.\n",
    "    \n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import re\n",
    "    import subprocess\n",
    "    shap_values = np.loadtxt(r'F:\\PatientHub\\MLModels\\DiabetesReadmission\\data\\shap_values.csv', delimiter=',') # load\n",
    "    print('Pre-calculated Shapley values loaded from disk.')        # print confirmation\n",
    "    #this needs to read from AML later\n",
    "    model = pickle.load(open(r'F:\\PatientHub\\MLModels\\DiabetesReadmission\\data\\model.pkl', 'rb'))\n",
    "    \n",
    "    # select shapley values for row\n",
    "    # reshape into column vector\n",
    "    # convert to pandas dataframe for easy plotting\n",
    "    for index, row in enumerate(input_df.iterrows()):\n",
    "        print(index)\n",
    "        \n",
    "        s_df = pd.DataFrame(shap_values[index, :][:-1], \n",
    "                            columns=['Approximate Local Contributions'],\n",
    "                            index=model.feature_names) # must use feature_names for consistent results\n",
    "        # print(shap_values)\n",
    "        # sort dataframe by shapley values and print values\n",
    "        s_df = s_df.sort_values(by='Approximate Local Contributions').T.reset_index()\n",
    "        #print(s_df.at['index'])\n",
    "        # drop additional column\n",
    "        s_df.at[0, 'index']= index\n",
    "        s_df.set_index(['index'], inplace = True)\n",
    "        #print(s_df, '\\n')\n",
    "        if index == 0:\n",
    "            df_return = s_df\n",
    "        else:\n",
    "            df_return = pd.concat([df_return, s_df])\n",
    "            #print(df_return)\n",
    "    return df_return\n",
    "\n",
    "    # plot top positive contributors for this row\n",
    "    #_= s_df.iloc[-5:,:].plot(kind='bar', title='Approximate Local Contributions', legend=False)\n",
    "    \n",
    "    # manually calculate sum of shapley values for row\n",
    "    #print('Shapley sum: ', s_df['Approximate Local Contributions'].sum() + shap_values[row.index[0], -1])\n",
    "    \n",
    "    # manually calculate actual model prediction before application of logit link function\n",
    "    #p = row['predict'].values[0]\n",
    "    #print('Model prediction: ', np.log(p/(1 - p))) # inverse logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row1 = train.iloc[[2]]\n",
    "# print(row1.index[0])\n",
    "localexp_df = shap_localexplain_df(train)\n",
    "# localexp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = urllib.parse.quote_plus(connection_string)\n",
    "engine = create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params).connect()\n",
    "\n",
    "localexp_df.to_sql('xiaoyongtest', con=engine, if_exists = 'replace', index_label = 'index', index = True, schema = 'ml', chunksize = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
